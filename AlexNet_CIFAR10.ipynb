{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "class LRN(nn.Module):\n",
    "    def __init__(self,local_size=1,alpha=1.0,beta=0.75,ACROSS_CHANNELS=True):\n",
    "        super(LRN,self).__init__()\n",
    "        self.ACROSS_CHANNELS=ACROSS_CHANNELS\n",
    "        self.alpha=alpha\n",
    "        self.beta=beta\n",
    "        if ACROSS_CHANNELS:\n",
    "            self.average=nn.AvgPool3d(kernel_size=(local_size,1,1),\n",
    "                                     stride=1,padding=(int((local_size-1.0)/2),0,0))\n",
    "        else:\n",
    "            self.average=nn.AvgPool2d(kernel_size=local_size,stride=1,padding=(int((local_size-1.0)/2),0,0))\n",
    "            self.alpha=alpha\n",
    "            self.beta=beta\n",
    "            \n",
    "    def forward(self,x):\n",
    "        if self.ACROSS_CHANNELS:\n",
    "            diver=x.pow(2).unsqueeze(1)\n",
    "            diver=self.average(diver).squeeze(1)\n",
    "            diver=diver.mul(self.alpha).add(1.0).pow(self.beta)\n",
    "        else:\n",
    "            diver=x.pow(2)\n",
    "            diver=self.average(diver)\n",
    "            diver=div.mul(self.alpha).add(1.0).pow(self.beta)\n",
    "        x=x.div(diver)\n",
    "        return x\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES=10\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self,num_classes=NUM_CLASSES):\n",
    "        super(AlexNet,self).__init__()\n",
    "        self.features=nn.Sequential(\n",
    "            nn.Conv2d(3,64,kernel_size=4,stride=2,padding=1),#有问题\n",
    "            nn.ReLU(inplace=True),\n",
    "            #LRN(local_size=3, alpha=0.0001, beta=0.75),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64,192,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #LRN(local_size=3, alpha=0.0001, beta=0.75),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(192,384,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384,256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256*2*2,4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "            \n",
    "    def forward(self,x):\n",
    "        x=self.features(x)\n",
    "        x=x.view(x.size(0),-1)#256*2*2\n",
    "        x=self.classifier(x)\n",
    "        return x\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "=====> epoch: 1/200\n",
      "train accuracy: 0.3262857142857143 loss: 862.7013040781021\n",
      "test====================\n",
      "test accuracy: 0.443\n",
      "\n",
      "=====> epoch: 2/200\n",
      "train accuracy: 0.5115918367346939 loss: 647.1862459182739\n",
      "test====================\n",
      "test accuracy: 0.562\n",
      "\n",
      "=====> epoch: 3/200\n",
      "train accuracy: 0.5974081632653061 loss: 547.086560189724\n",
      "test====================\n",
      "test accuracy: 0.582\n",
      "\n",
      "=====> epoch: 4/200\n",
      "train accuracy: 0.6541632653061225 loss: 474.4081481695175\n",
      "test====================\n",
      "test accuracy: 0.628\n",
      "\n",
      "=====> epoch: 5/200\n",
      "train accuracy: 0.7043877551020408 loss: 409.82465732097626\n",
      "test====================\n",
      "test accuracy: 0.667\n",
      "\n",
      "=====> epoch: 6/200\n",
      "train accuracy: 0.7450204081632653 loss: 353.8947731554508\n",
      "test====================\n",
      "test accuracy: 0.686\n",
      "\n",
      "=====> epoch: 7/200\n",
      "train accuracy: 0.7884489795918367 loss: 296.36785411834717\n",
      "test====================\n",
      "test accuracy: 0.708\n",
      "\n",
      "=====> epoch: 8/200\n",
      "train accuracy: 0.8235102040816327 loss: 246.83896562457085\n",
      "test====================\n",
      "test accuracy: 0.677\n",
      "\n",
      "=====> epoch: 9/200\n",
      "train accuracy: 0.8593673469387755 loss: 198.62453903257847\n",
      "test====================\n",
      "test accuracy: 0.695\n",
      "\n",
      "=====> epoch: 10/200\n",
      "train accuracy: 0.8878775510204082 loss: 158.6512036472559\n",
      "test====================\n",
      "test accuracy: 0.684\n",
      "\n",
      "=====> epoch: 11/200\n",
      "train accuracy: 0.9103877551020408 loss: 127.57474453374743\n",
      "test====================\n",
      "test accuracy: 0.698\n",
      "\n",
      "=====> epoch: 12/200\n",
      "train accuracy: 0.9284081632653062 loss: 101.7228038944304\n",
      "test====================\n",
      "test accuracy: 0.707\n",
      "\n",
      "=====> epoch: 13/200\n",
      "train accuracy: 0.9394081632653061 loss: 84.9093462228775\n",
      "test====================\n",
      "test accuracy: 0.699\n",
      "\n",
      "=====> epoch: 14/200\n",
      "train accuracy: 0.9508571428571428 loss: 71.09466641768813\n",
      "test====================\n",
      "test accuracy: 0.691\n",
      "\n",
      "=====> epoch: 15/200\n",
      "train accuracy: 0.9592448979591837 loss: 58.42325892299414\n",
      "test====================\n",
      "test accuracy: 0.694\n",
      "\n",
      "=====> epoch: 16/200\n",
      "train accuracy: 0.9632448979591837 loss: 52.83061398752034\n",
      "test====================\n",
      "test accuracy: 0.703\n",
      "\n",
      "=====> epoch: 17/200\n",
      "train accuracy: 0.9646326530612245 loss: 52.03895447868854\n",
      "test====================\n",
      "test accuracy: 0.695\n",
      "\n",
      "=====> epoch: 18/200\n",
      "train accuracy: 0.969469387755102 loss: 43.74200358707458\n",
      "test====================\n",
      "test accuracy: 0.706\n",
      "\n",
      "=====> epoch: 19/200\n",
      "train accuracy: 0.9739387755102041 loss: 37.934761974960566\n",
      "test====================\n",
      "test accuracy: 0.697\n",
      "\n",
      "=====> epoch: 20/200\n",
      "train accuracy: 0.9723469387755102 loss: 39.561832738574594\n",
      "test====================\n",
      "test accuracy: 0.71\n",
      "\n",
      "=====> epoch: 21/200\n",
      "train accuracy: 0.9760408163265306 loss: 34.509373747045174\n",
      "test====================\n",
      "test accuracy: 0.718\n",
      "\n",
      "=====> epoch: 22/200\n",
      "train accuracy: 0.9771836734693877 loss: 33.06440288387239\n",
      "test====================\n",
      "test accuracy: 0.723\n",
      "\n",
      "=====> epoch: 23/200\n",
      "train accuracy: 0.978265306122449 loss: 32.45315467566252\n",
      "test====================\n",
      "test accuracy: 0.718\n",
      "\n",
      "=====> epoch: 24/200\n",
      "train accuracy: 0.9792448979591837 loss: 30.076461746590212\n",
      "test====================\n",
      "test accuracy: 0.701\n",
      "\n",
      "=====> epoch: 25/200\n",
      "train accuracy: 0.9805102040816327 loss: 28.711750775575638\n",
      "test====================\n",
      "test accuracy: 0.722\n",
      "\n",
      "=====> epoch: 26/200\n",
      "train accuracy: 0.9808775510204082 loss: 27.893042718293145\n",
      "test====================\n",
      "test accuracy: 0.705\n",
      "\n",
      "=====> epoch: 27/200\n",
      "train accuracy: 0.9801224489795919 loss: 27.863413115148433\n",
      "test====================\n",
      "test accuracy: 0.707\n",
      "\n",
      "=====> epoch: 28/200\n",
      "train accuracy: 0.9818571428571429 loss: 27.6735546158161\n",
      "test====================\n",
      "test accuracy: 0.727\n",
      "\n",
      "=====> epoch: 29/200\n",
      "train accuracy: 0.9853469387755102 loss: 21.43025824497454\n",
      "test====================\n",
      "test accuracy: 0.718\n",
      "\n",
      "=====> epoch: 30/200\n",
      "train accuracy: 0.9827142857142858 loss: 25.117825029650703\n",
      "test====================\n",
      "test accuracy: 0.73\n",
      "\n",
      "=====> epoch: 31/200\n",
      "train accuracy: 0.9834897959183674 loss: 23.983887187205255\n",
      "test====================\n",
      "test accuracy: 0.731\n",
      "\n",
      "=====> epoch: 32/200\n",
      "train accuracy: 0.9840816326530613 loss: 22.619990218663588\n",
      "test====================\n",
      "test accuracy: 0.719\n",
      "\n",
      "=====> epoch: 33/200\n",
      "train accuracy: 0.985265306122449 loss: 22.512212526751682\n",
      "test====================\n",
      "test accuracy: 0.722\n",
      "\n",
      "=====> epoch: 34/200\n",
      "train accuracy: 0.9840204081632653 loss: 23.787263200385496\n",
      "test====================\n",
      "test accuracy: 0.714\n",
      "\n",
      "=====> epoch: 35/200\n",
      "train accuracy: 0.9864489795918367 loss: 19.175682019558735\n",
      "test====================\n",
      "test accuracy: 0.714\n",
      "\n",
      "=====> epoch: 36/200\n",
      "train accuracy: 0.9865102040816327 loss: 19.154018886387348\n",
      "test====================\n",
      "test accuracy: 0.705\n",
      "\n",
      "=====> epoch: 37/200\n",
      "train accuracy: 0.9858775510204082 loss: 20.673153207753785\n",
      "test====================\n",
      "test accuracy: 0.728\n",
      "\n",
      "=====> epoch: 38/200\n",
      "train accuracy: 0.9871632653061224 loss: 18.57696015329566\n",
      "test====================\n",
      "test accuracy: 0.705\n",
      "\n",
      "=====> epoch: 39/200\n",
      "train accuracy: 0.9864693877551021 loss: 19.8285898604081\n",
      "test====================\n",
      "test accuracy: 0.713\n",
      "\n",
      "=====> epoch: 40/200\n",
      "train accuracy: 0.9858367346938776 loss: 20.710375659633428\n",
      "test====================\n",
      "test accuracy: 0.706\n",
      "\n",
      "=====> epoch: 41/200\n",
      "train accuracy: 0.9883469387755102 loss: 18.19720920929103\n",
      "test====================\n",
      "test accuracy: 0.735\n",
      "\n",
      "=====> epoch: 42/200\n",
      "train accuracy: 0.9888979591836735 loss: 17.981686671322677\n",
      "test====================\n",
      "test accuracy: 0.706\n",
      "\n",
      "=====> epoch: 43/200\n",
      "train accuracy: 0.9874897959183674 loss: 17.97032623772975\n",
      "test====================\n",
      "test accuracy: 0.729\n",
      "\n",
      "=====> epoch: 44/200\n",
      "train accuracy: 0.9910816326530613 loss: 14.15119126366335\n",
      "test====================\n",
      "test accuracy: 0.702\n",
      "\n",
      "=====> epoch: 45/200\n",
      "train accuracy: 0.9875714285714285 loss: 18.982842437690124\n",
      "test====================\n",
      "test accuracy: 0.715\n",
      "\n",
      "=====> epoch: 46/200\n",
      "train accuracy: 0.9878163265306122 loss: 18.14212392491754\n",
      "test====================\n",
      "test accuracy: 0.721\n",
      "\n",
      "=====> epoch: 47/200\n",
      "train accuracy: 0.9896530612244898 loss: 14.980840693344362\n",
      "test====================\n",
      "test accuracy: 0.722\n",
      "\n",
      "=====> epoch: 48/200\n",
      "train accuracy: 0.988204081632653 loss: 16.816580381128006\n",
      "test====================\n",
      "test accuracy: 0.728\n",
      "\n",
      "=====> epoch: 49/200\n",
      "train accuracy: 0.9890204081632653 loss: 15.978157574660145\n",
      "test====================\n",
      "test accuracy: 0.72\n",
      "\n",
      "=====> epoch: 50/200\n",
      "train accuracy: 0.9916530612244898 loss: 13.127680429417524\n",
      "test====================\n",
      "test accuracy: 0.724\n",
      "\n",
      "=====> epoch: 51/200\n",
      "train accuracy: 0.9910204081632653 loss: 13.40658779052319\n",
      "test====================\n",
      "test accuracy: 0.713\n",
      "\n",
      "=====> epoch: 52/200\n",
      "train accuracy: 0.989938775510204 loss: 15.974690355651546\n",
      "test====================\n",
      "test accuracy: 0.724\n",
      "\n",
      "=====> epoch: 53/200\n",
      "train accuracy: 0.9894693877551021 loss: 16.301330363436136\n",
      "test====================\n",
      "test accuracy: 0.708\n",
      "\n",
      "=====> epoch: 54/200\n",
      "train accuracy: 0.9901224489795918 loss: 14.937610276974738\n",
      "test====================\n",
      "test accuracy: 0.731\n",
      "\n",
      "=====> epoch: 55/200\n",
      "train accuracy: 0.9911632653061224 loss: 13.584701932733878\n",
      "test====================\n",
      "test accuracy: 0.722\n",
      "\n",
      "=====> epoch: 56/200\n",
      "train accuracy: 0.9900816326530613 loss: 16.644285967282485\n",
      "test====================\n",
      "test accuracy: 0.712\n",
      "\n",
      "=====> epoch: 57/200\n",
      "train accuracy: 0.9914693877551021 loss: 12.929549620719627\n",
      "test====================\n",
      "test accuracy: 0.705\n",
      "\n",
      "=====> epoch: 58/200\n",
      "train accuracy: 0.9911428571428571 loss: 13.990293809096329\n",
      "test====================\n",
      "test accuracy: 0.712\n",
      "\n",
      "=====> epoch: 59/200\n",
      "train accuracy: 0.9919795918367347 loss: 11.718257441520109\n",
      "test====================\n",
      "test accuracy: 0.7\n",
      "\n",
      "=====> epoch: 60/200\n",
      "train accuracy: 0.9921224489795918 loss: 11.83414299019205\n",
      "test====================\n",
      "test accuracy: 0.718\n",
      "\n",
      "=====> epoch: 61/200\n",
      "train accuracy: 0.9907551020408163 loss: 14.092726555405534\n",
      "test====================\n",
      "test accuracy: 0.702\n",
      "\n",
      "=====> epoch: 62/200\n",
      "train accuracy: 0.9906122448979592 loss: 14.664963934599655\n",
      "test====================\n",
      "test accuracy: 0.711\n",
      "\n",
      "=====> epoch: 63/200\n",
      "train accuracy: 0.9929795918367347 loss: 11.073701803827134\n",
      "test====================\n",
      "test accuracy: 0.707\n",
      "\n",
      "=====> epoch: 64/200\n",
      "train accuracy: 0.9908367346938776 loss: 14.695464763470227\n",
      "test====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.718\n",
      "\n",
      "=====> epoch: 65/200\n",
      "train accuracy: 0.9928775510204082 loss: 11.166395108200959\n",
      "test====================\n",
      "test accuracy: 0.711\n",
      "\n",
      "=====> epoch: 66/200\n",
      "train accuracy: 0.9913061224489796 loss: 13.003872280125506\n",
      "test====================\n",
      "test accuracy: 0.726\n",
      "\n",
      "=====> epoch: 67/200\n",
      "train accuracy: 0.9917142857142857 loss: 12.453700906044105\n",
      "test====================\n",
      "test accuracy: 0.72\n",
      "\n",
      "=====> epoch: 68/200\n",
      "train accuracy: 0.9922244897959184 loss: 12.906551488966215\n",
      "test====================\n",
      "test accuracy: 0.722\n",
      "\n",
      "=====> epoch: 69/200\n",
      "train accuracy: 0.9929591836734694 loss: 11.114897075924091\n",
      "test====================\n",
      "test accuracy: 0.723\n",
      "\n",
      "=====> epoch: 70/200\n",
      "train accuracy: 0.9930816326530613 loss: 10.442134689910745\n",
      "test====================\n",
      "test accuracy: 0.723\n",
      "\n",
      "=====> epoch: 71/200\n",
      "train accuracy: 0.9918367346938776 loss: 12.389458281046245\n",
      "test====================\n",
      "test accuracy: 0.721\n",
      "\n",
      "=====> epoch: 72/200\n",
      "train accuracy: 0.9921224489795918 loss: 12.526631698783603\n",
      "test====================\n",
      "test accuracy: 0.723\n",
      "\n",
      "=====> epoch: 73/200\n",
      "train accuracy: 0.9923673469387755 loss: 12.069310886450694\n",
      "test====================\n",
      "test accuracy: 0.718\n",
      "\n",
      "=====> epoch: 74/200\n",
      "train accuracy: 0.9918163265306122 loss: 13.542302749803639\n",
      "test====================\n",
      "test accuracy: 0.725\n",
      "\n",
      "=====> epoch: 75/200\n",
      "train accuracy: 0.9980816326530613 loss: 3.1377719547108427\n",
      "test====================\n",
      "test accuracy: 0.716\n",
      "\n",
      "=====> epoch: 76/200\n",
      "train accuracy: 0.9989183673469387 loss: 1.6372104700776617\n",
      "test====================\n",
      "test accuracy: 0.721\n",
      "\n",
      "=====> epoch: 77/200\n",
      "train accuracy: 0.9985918367346939 loss: 2.213228125077194\n",
      "test====================\n",
      "test accuracy: 0.716\n",
      "\n",
      "=====> epoch: 78/200\n",
      "train accuracy: 0.9992040816326531 loss: 1.2085124155101425\n",
      "test====================\n",
      "test accuracy: 0.729\n",
      "\n",
      "=====> epoch: 79/200\n",
      "train accuracy: 0.9977755102040816 loss: 4.1665870882087575\n",
      "test====================\n",
      "test accuracy: 0.729\n",
      "\n",
      "=====> epoch: 80/200\n",
      "train accuracy: 0.9988571428571429 loss: 1.5104829027734468\n",
      "test====================\n",
      "test accuracy: 0.73\n",
      "\n",
      "=====> epoch: 81/200\n",
      "train accuracy: 0.9966122448979592 loss: 5.479946856572724\n",
      "test====================\n",
      "test accuracy: 0.738\n",
      "\n",
      "=====> epoch: 82/200\n",
      "train accuracy: 0.9984489795918368 loss: 2.58248214426294\n",
      "test====================\n",
      "test accuracy: 0.734\n",
      "\n",
      "=====> epoch: 83/200\n",
      "train accuracy: 0.9990204081632653 loss: 1.7601003907730046\n",
      "test====================\n",
      "test accuracy: 0.725\n",
      "\n",
      "=====> epoch: 84/200\n",
      "train accuracy: 0.9985102040816326 loss: 2.55821528047602\n",
      "test====================\n",
      "test accuracy: 0.726\n",
      "\n",
      "=====> epoch: 85/200\n",
      "train accuracy: 0.9974081632653061 loss: 4.402680532082513\n",
      "test====================\n",
      "test accuracy: 0.734\n",
      "\n",
      "=====> epoch: 86/200\n",
      "train accuracy: 0.9986326530612245 loss: 2.117134851095898\n",
      "test====================\n",
      "test accuracy: 0.726\n",
      "\n",
      "=====> epoch: 87/200\n",
      "train accuracy: 0.9975918367346939 loss: 3.88055925744834\n",
      "test====================\n",
      "test accuracy: 0.731\n",
      "\n",
      "=====> epoch: 88/200\n",
      "train accuracy: 0.998734693877551 loss: 2.3898760643575088\n",
      "test====================\n",
      "test accuracy: 0.723\n",
      "\n",
      "=====> epoch: 89/200\n",
      "train accuracy: 0.9989591836734694 loss: 1.8202563536455045\n",
      "test====================\n",
      "test accuracy: 0.716\n",
      "\n",
      "=====> epoch: 90/200\n",
      "train accuracy: 0.9965510204081632 loss: 5.365905155173039\n",
      "test====================\n",
      "test accuracy: 0.712\n",
      "\n",
      "=====> epoch: 91/200\n",
      "train accuracy: 0.9986122448979592 loss: 2.5731579169055294\n",
      "test====================\n",
      "test accuracy: 0.721\n",
      "\n",
      "=====> epoch: 92/200\n",
      "train accuracy: 0.9976122448979592 loss: 3.633171488116176\n",
      "test====================\n",
      "test accuracy: 0.726\n",
      "\n",
      "=====> epoch: 93/200\n",
      "train accuracy: 0.9988163265306123 loss: 1.8841285931537186\n",
      "test====================\n",
      "test accuracy: 0.718\n",
      "\n",
      "=====> epoch: 94/200\n",
      "train accuracy: 0.9985918367346939 loss: 2.212576782654196\n",
      "test====================\n",
      "test accuracy: 0.721\n",
      "\n",
      "=====> epoch: 95/200\n",
      "train accuracy: 0.9964897959183674 loss: 6.058900857486606\n",
      "test====================\n",
      "test accuracy: 0.723\n",
      "\n",
      "=====> epoch: 96/200\n",
      "train accuracy: 0.9987551020408163 loss: 2.1709542798685106\n",
      "test====================\n",
      "test accuracy: 0.726\n",
      "\n",
      "=====> epoch: 97/200\n",
      "train accuracy: 0.997734693877551 loss: 3.718480478713218\n",
      "test====================\n",
      "test accuracy: 0.721\n",
      "\n",
      "=====> epoch: 98/200\n",
      "train accuracy: 0.998734693877551 loss: 1.9543359802586338\n",
      "test====================\n",
      "test accuracy: 0.731\n",
      "\n",
      "=====> epoch: 99/200\n",
      "train accuracy: 0.9981224489795918 loss: 2.8498444458494987\n",
      "test====================\n",
      "test accuracy: 0.714\n",
      "\n",
      "=====> epoch: 100/200\n",
      "train accuracy: 0.9984693877551021 loss: 2.808413682081209\n",
      "test====================\n",
      "test accuracy: 0.705\n",
      "===> BEST ACC. PERFORMANCE: 73.800%\n",
      "4522.142513990402 Final time\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import sampler\n",
    "import torch.backends.cudnn as audnn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "writer=SummaryWriter()\n",
    "\n",
    "lr=2.5e-4\n",
    "epoch=100\n",
    "trainBatchSize=100\n",
    "testBatchSize=100\n",
    "NUM_TRAIN = 49000\n",
    "class Solver():\n",
    "    def __init__(self):\n",
    "        self.model=None\n",
    "        self.lr=lr\n",
    "        self.epochs=epoch\n",
    "        self.train_batch_size=trainBatchSize\n",
    "        self.test_batch_size=testBatchSize\n",
    "        self.optimizer=None\n",
    "        self.device=None\n",
    "        self.cuda=torch.cuda.is_available()\n",
    "        self.train_loader=None\n",
    "        self.val_loader=None\n",
    "        self.test_loader=None\n",
    "        self.scheduler=None\n",
    "        self.criterion =None\n",
    "        \n",
    "    def load_data(self):\n",
    "        train_transform=transforms.Compose([transforms.RandomHorizontalFlip()，transforms.ToTensor()，transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "        test_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "        train_set=torchvision.datasets.CIFAR10(root='./data',train=True,download=True,transform=train_transform)\n",
    "        self.train_loader=torch.utils.data.DataLoader(dataset=train_set,batch_size=self.train_batch_size,sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "        val_set=torchvision.datasets.CIFAR10(root='./data',train=True,download=True,transform=train_transform)\n",
    "        self.val_loader=torch.utils.data.DataLoader(dataset=train_set,batch_size=self.train_batch_size,sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "        test_set=torchvision.datasets.CIFAR10(root='./data',train=False,download=True,transform=test_transform)\n",
    "        self.test_loader=torch.utils.data.DataLoader(dataset=test_set,batch_size=self.test_batch_size,sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "        \n",
    "    def load_model(self):\n",
    "        if self.cuda:\n",
    "            self.device=torch.device('cuda')#missed\n",
    "        else:\n",
    "            slf.device=torch.device('cpu')\n",
    "            \n",
    "        self.model=AlexNet().to(self.device)\n",
    "        self.optimizer=optim.Adam(self.model.parameters(),lr=self.lr)\n",
    "        self.scheduler=optim.lr_scheduler.MultiStepLR(self.optimizer,milestones=[75,150],gamma=0.5)\n",
    "        self.criterion =nn.CrossEntropyLoss().to(self.device)\n",
    "        \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        train_loss=0\n",
    "        train_correct=0\n",
    "        total=0\n",
    "        \n",
    "        for batch_num,(data,target) in enumerate(self.train_loader):\n",
    "            data,target=data.to(self.device),target.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output=self.model(data)\n",
    "            loss=self.criterion(output,target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "            prediction=torch.max(output,1)\n",
    "            total+=target.size(0)\n",
    "            train_correct += np.sum(prediction[1].cpu().numpy() == target.cpu().numpy())\n",
    "            #print('loss:',loss.item(),'accuracy:',train_correct/total)\n",
    "            \n",
    "        return train_loss , train_correct/total\n",
    "    \n",
    "    def test(self):\n",
    "        print('test====================')\n",
    "        self.model.eval()\n",
    "        test_loss=0\n",
    "        test_correct=0\n",
    "        total=0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_num, (data, target) in enumerate(self.val_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output=self.model(data)\n",
    "                loss=self.criterion(output,target)\n",
    "                test_loss+=loss.item()\n",
    "                prediction=torch.max(output,1)\n",
    "                total+=target.size(0)\n",
    "                test_correct += np.sum(prediction[1].cpu().numpy() == target.cpu().numpy())\n",
    "                #print ('test accuracy:',test_correct/total)\n",
    "        return test_loss,test_correct/total\n",
    "    \n",
    "    def run(self,writer):\n",
    "        self.load_data()\n",
    "        self.load_model()\n",
    "        accuracy=0\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.scheduler.step(epoch)\n",
    "            print(\"\\n=====> epoch: %d/200\" % epoch)\n",
    "            train_loss,train_result=self.train()\n",
    "            print ('train accuracy:',train_result,'loss:',train_loss)\n",
    "            test_result=self.test()\n",
    "            accuracy=max(accuracy,test_result[1])\n",
    "            print('test accuracy:',test_result[1])\n",
    "            writer.add_scalar('loss_value',train_loss,epoch)\n",
    "            if epoch == self.epochs:\n",
    "                 print(\"===> BEST ACC. PERFORMANCE: %.3f%%\" % (accuracy * 100))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def final_test(self):\n",
    "        self.model.eval()\n",
    "        for x, y in  self.test_loader:\n",
    "            x=x.to(self.device)\n",
    "            y=y.to(self.device)\n",
    "            scores=self.model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('FINNAL ACCURACY:',acc)\n",
    "            \n",
    "            \n",
    "\n",
    "solver=Solver()\n",
    "solver.run(writer)  \n",
    "writer.close()\n",
    "end=time.time()\n",
    "print (end-start,\"Final time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "FINNAL ACCURACY: 0.7183\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model=AlexNet()\n",
    "\n",
    "    \n",
    "\n",
    "solver.model.eval()\n",
    "test_transform=transforms.Compose( [transforms.ToTensor()])\n",
    "num_correct=0\n",
    "num_samples=0\n",
    "test_set=torchvision.datasets.CIFAR10(root='./data',train=False,download=True,transform=test_transform) \n",
    "test_loader=torch.utils.data.DataLoader(dataset=test_set,batch_size=100)\n",
    "for x, y in  test_loader:\n",
    "                x=x.to(solver.device)\n",
    "                y=y.to(solver.device)\n",
    "                scores=solver.model(x)\n",
    "                _, preds = scores.max(1)\n",
    "                num_correct += (preds == y).sum()\n",
    "                num_samples += preds.size(0)\n",
    "                acc = float(num_correct) / num_samples\n",
    "x=x.to(torch.device(\"cpu\"))\n",
    "    \n",
    "print('FINNAL ACCURACY:',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "train_loss=0\n",
    "train_correct=0\n",
    "total=0\n",
    "w=SummaryWriter(comment='alexnet')  \n",
    "for batch_num,(data,target) in enumerate(solver.val_loader):\n",
    "            data,target=data.to(solver.device),target.to(solver.device)\n",
    "            solver.optimizer.zero_grad()\n",
    "            output=solver.model(data)\n",
    "            loss=solver.criterion(output,target)\n",
    "            loss.backward()\n",
    "            solver.optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "            prediction=torch.max(output,1)\n",
    "            total+=target.size(0)\n",
    "            train_correct += np.sum(prediction[1].cpu().numpy() == target.cpu().numpy())       \n",
    "data = data.to(torch.device(\"cpu\"))\n",
    "model=AlexNet()\n",
    "w.add_graph(model,(data,))\n",
    "w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.bn = nn.BatchNorm2d(20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(self.conv1(x), 2)\n",
    "        x = F.relu(x) + F.relu(-x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = self.bn(x)\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "dummy_input = torch.rand(13, 1, 28, 28)\n",
    "\n",
    "model = Net1()\n",
    "with SummaryWriter(comment='Net1') as w:\n",
    "    w.add_graph(model, (dummy_input,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
